{"pageProps":{"tutorialData":{"contentHtml":"<p>In this tutorial, we will walk you through how to get started with GEM, how to load and inspect data, how to finetune a baseline model, and how to generate predictions.\nThroughout this tutorial, we will focus on the CommonGen task, but we will note\nwhat changes to make to use another of the GEM datasets.</p>\n<h2 id=\"table-of-contents\">Table of Contents</h2>\n<ul>\n<li><a href=\"#preliminaries\">Preliminaries</a></li>\n<li><a href=\"#loading-the-data\">Loading the Data</a></li>\n<li><a href=\"#finetuning-a-pretrained-model\">Finetuning a pretrained model</a></li>\n<li><a href=\"#generating-and-evaluating-predictions\">Generating and evaluating Predictions</a></li>\n<li><a href=\"#generating-and-submitting-test-predictions\">Generating and Submitting Test Predictions</a></li>\n<li><a href=\"#evaluating-your-submission-file-with-the-gem-evaluation-framework\">Evaluating your submission file with the GEM evaluation framework.</a></li>\n</ul>\n<h2 id=\"preliminaries\">Preliminaries</h2>\n<p>This tutorial uses PyTorch and the HuggingFace infrastructure to finetune models. You need to install the following dependencies:</p>\n<pre><code>pip install datasets\npip install rouge_score\npip install sentencepiece\npip install transformers\n</code></pre>\n<p>The device management can be done via the following imports:</p>\n<pre><code class=\"hljs language-py\"><span class=\"hljs-comment\"># Device and fp16 management.</span>\n<span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span> packaging <span class=\"hljs-keyword\">import</span> version\n<span class=\"hljs-keyword\">if</span> version.parse(torch.__version__) &#x3C; version.parse(<span class=\"hljs-string\">\"1.6\"</span>):\n    <span class=\"hljs-keyword\">from</span> .file_utils <span class=\"hljs-keyword\">import</span> is_apex_available\n    <span class=\"hljs-keyword\">if</span> is_apex_available():\n        <span class=\"hljs-keyword\">from</span> apex <span class=\"hljs-keyword\">import</span> amp\n    _use_apex = <span class=\"hljs-literal\">True</span>\n<span class=\"hljs-keyword\">else</span>:\n    _use_native_amp = <span class=\"hljs-literal\">True</span>\n    <span class=\"hljs-keyword\">from</span> torch.cuda.amp <span class=\"hljs-keyword\">import</span> autocast</code></pre>\n<h2 id=\"loading-the-data\">Loading the Data</h2>\n<p>We will be using <a href=\"https://huggingface.co/docs/datasets/\">HuggingFace datasets</a> for this tutorial, but the GEM datasets are available in <a href=\"https://www.tensorflow.org/datasets\">TFDS</a> as well.</p>\n<p>You can load and inspect datasets like this:</p>\n<pre><code class=\"hljs language-python\">>> <span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\">import</span> load_dataset\n>> data = load_dataset(<span class=\"hljs-string\">\"common_gen\"</span>)\n>> data\n\nDatasetDict({\n    train: Dataset({\n        features: [<span class=\"hljs-string\">'concept_set_idx'</span>, <span class=\"hljs-string\">'concepts'</span>, <span class=\"hljs-string\">'target'</span>],\n        num_rows: <span class=\"hljs-number\">67389</span>\n    })\n    validation: Dataset({\n        features: [<span class=\"hljs-string\">'concept_set_idx'</span>, <span class=\"hljs-string\">'concepts'</span>, <span class=\"hljs-string\">'target'</span>],\n        num_rows: <span class=\"hljs-number\">4018</span>\n    })\n    test: Dataset({\n        features: [<span class=\"hljs-string\">'concept_set_idx'</span>, <span class=\"hljs-string\">'concepts'</span>, <span class=\"hljs-string\">'target'</span>],\n        num_rows: <span class=\"hljs-number\">1497</span>\n    })\n})</code></pre>\n<p>Now let's look at a single example:</p>\n<pre><code class=\"hljs language-python\">>> data[<span class=\"hljs-string\">'train'</span>][<span class=\"hljs-number\">0</span>]\n\n{<span class=\"hljs-string\">'concept_set_idx'</span>: <span class=\"hljs-number\">0</span>,\n <span class=\"hljs-string\">'concepts'</span>: [<span class=\"hljs-string\">'ski'</span>, <span class=\"hljs-string\">'mountain'</span>, <span class=\"hljs-string\">'skier'</span>],\n <span class=\"hljs-string\">'target'</span>: <span class=\"hljs-string\">'Skier skis down the mountain'</span>}</code></pre>\n<p>CommonGen is a task that asks for the production of a sentence (<code>target</code>) from a set of concepts (<code>concepts</code>). Since one concept set can generate multiple meaningful sentences, the example also includes a unique identifier (<code>concept_set_idx</code>) so that multiple references can be linked to an input.</p>\n<p>Next, let's define utility functions that can generate batches of (tokenized) examples which we can use during training.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">construct_input_for_batch</span>(<span class=\"hljs-params\">batch</span>):</span>\n  <span class=\"hljs-string\">\"\"\"\n  Function that takes a batch from a dataset and constructs the corresponding\n  input string.\n  \"\"\"</span>\n  source = [<span class=\"hljs-string\">' '</span>.join(concepts) <span class=\"hljs-keyword\">for</span> concepts <span class=\"hljs-keyword\">in</span> batch [<span class=\"hljs-string\">\"concepts\"</span>]]\n  target = batch[<span class=\"hljs-string\">\"target\"</span>]\n  <span class=\"hljs-keyword\">return</span> source, target\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">batch_tokenize</span>(<span class=\"hljs-params\">dataset_batch, tokenizer, dataset_name, decoder_max_length=<span class=\"hljs-number\">32</span></span>):</span>\n  <span class=\"hljs-string\">\"\"\"\n  Construct the batch (source, target) and run them through a tokenizer.\n  \"\"\"</span>\n  source, target = construct_input_for_batch(dataset_batch, dataset_name)\n  res = {\n      <span class=\"hljs-string\">\"input_ids\"</span>: tokenizer(source)[<span class=\"hljs-string\">\"input_ids\"</span>],\n      <span class=\"hljs-string\">\"labels\"</span>: tokenizer(\n          target,\n          padding=<span class=\"hljs-string\">'max_length'</span>,\n          truncation=<span class=\"hljs-literal\">True</span>,\n          max_length=decoder_max_length\n      )[<span class=\"hljs-string\">\"input_ids\"</span>],\n  }\n  <span class=\"hljs-keyword\">return</span> res</code></pre>\n<p>All we need to do now to preprocess the dataset is to call <code>batch_tokenize</code> on it. For our example, we are using BART-base as a model and we need to load the corresponding tokenizer:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">'facebook/bart-base'</span>)\n\ntrain_data_tokenized = data[<span class=\"hljs-string\">'train'</span>].<span class=\"hljs-built_in\">map</span>(\n  <span class=\"hljs-keyword\">lambda</span> batch: batch_tokenize(batch, tokenizer, DATASET_NAME, decoder_max_length=DECODER_MAX_LENGTH),\n  batched=<span class=\"hljs-literal\">True</span>\n)\nvalid_data_tokenized = data[<span class=\"hljs-string\">'validation'</span>].<span class=\"hljs-built_in\">map</span>(\n  <span class=\"hljs-keyword\">lambda</span> batch: batch_tokenize(batch, tokenizer, DATASET_NAME, decoder_max_length=DECODER_MAX_LENGTH),\n  batched=<span class=\"hljs-literal\">True</span>\n)</code></pre>\n<h2 id=\"finetuning-a-pretrained-model\">Finetuning a pretrained model</h2>\n<p>We can now utilize the preprocessed data to finetune a model. To do so, we will utilize the <a href=\"https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\">Trainer API</a> by defining a <code>Seq2SeqTrainer</code> class that handles gradient updates, model selection, and evaluation for us.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span> Trainer, TrainingArguments\n\n<span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">Seq2SeqTrainer</span>(<span class=\"hljs-params\">Trainer</span>):</span>\n  <span class=\"hljs-string\">\"\"\"Class to finetune a Seq2Seq model.\"\"\"</span>\n  <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">__init__</span>(<span class=\"hljs-params\">\n    self,\n    num_beams=<span class=\"hljs-number\">4</span>,\n    max_length=<span class=\"hljs-number\">32</span>,\n    *args, **kwargs\n  </span>):</span>\n    <span class=\"hljs-built_in\">super</span>().__init__(*args, **kwargs)\n    self.num_beams = num_beams\n    self.max_length = max_length\n\n  <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">prediction_step</span>(<span class=\"hljs-params\">self, model, inputs, prediction_loss_only, ignore_keys=<span class=\"hljs-literal\">None</span></span>):</span>\n    <span class=\"hljs-string\">\"\"\"\n    Runs the model to either generate a sequence and/or compute the loss.\n    \"\"\"</span>\n    has_labels = <span class=\"hljs-built_in\">all</span>(inputs.get(k) <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\">for</span> k <span class=\"hljs-keyword\">in</span> self.label_names)\n    inputs = self._prepare_inputs(inputs)\n    <span class=\"hljs-comment\"># Compute loss with labels first.</span>\n    <span class=\"hljs-keyword\">with</span> torch.no_grad():\n      <span class=\"hljs-keyword\">if</span> self.args.fp16 <span class=\"hljs-keyword\">and</span> _use_native_amp:\n        <span class=\"hljs-keyword\">with</span> autocast():\n          outputs = model(**inputs)\n      <span class=\"hljs-keyword\">else</span>:\n        outputs = model(**inputs)\n      <span class=\"hljs-keyword\">if</span> has_labels:\n        loss = outputs[<span class=\"hljs-number\">0</span>].mean().detach()\n      <span class=\"hljs-keyword\">else</span>:\n        loss = <span class=\"hljs-literal\">None</span>\n    <span class=\"hljs-comment\"># If we're only computing the conditional log-likelihood, return.</span>\n    <span class=\"hljs-keyword\">if</span> prediction_loss_only:\n      <span class=\"hljs-keyword\">return</span> (loss, <span class=\"hljs-literal\">None</span>, <span class=\"hljs-literal\">None</span>)\n    <span class=\"hljs-comment\"># Otherwise run model.generate() to get predictions.</span>\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">isinstance</span>(model, torch.nn.DataParallel):\n      preds = model.module.generate(\n        input_ids=inputs[<span class=\"hljs-string\">'input_ids'</span>],\n        attention_mask=inputs[<span class=\"hljs-string\">'attention_mask'</span>],\n        num_beams=self.num_beams,\n        max_length=self.max_length,\n      )\n    <span class=\"hljs-keyword\">else</span>:\n      preds = model.generate(\n        input_ids=inputs[<span class=\"hljs-string\">'input_ids'</span>],\n        attention_mask=inputs[<span class=\"hljs-string\">'attention_mask'</span>],\n        num_beams=self.num_beams,\n        max_length=self.max_length,\n      )\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(preds) == <span class=\"hljs-number\">1</span>:\n      preds = preds[<span class=\"hljs-number\">0</span>]\n    <span class=\"hljs-comment\"># Pad predictions if necessary so they can be concatenated across batches.</span>\n    <span class=\"hljs-keyword\">if</span> preds.shape[-<span class=\"hljs-number\">1</span>] &#x3C; self.max_length:\n      preds = torch.nn.functional.pad(\n        preds, (<span class=\"hljs-number\">0</span>, self.max_length-preds.shape[-<span class=\"hljs-number\">1</span>]),\n        mode=<span class=\"hljs-string\">'constant'</span>,\n        value=self.tokenizer.pad_token_id\n      )\n    <span class=\"hljs-comment\"># Post-process labels.</span>\n    <span class=\"hljs-keyword\">if</span> has_labels:\n      labels = inputs.get(<span class=\"hljs-string\">'labels'</span>)\n    <span class=\"hljs-keyword\">else</span>:\n      labels = <span class=\"hljs-literal\">None</span>\n    <span class=\"hljs-keyword\">return</span> (loss, preds, labels)</code></pre>\n<p>To improve model selection, let's pick the model that has the best test performance on ROUGE-2, a metric that is typically associated with higher fluency. We can do this by constructing a function that returns a function that computes the score and we only have to pass it to our trainer.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\">import</span> load_metric\n\nrouge_scorer = load_metric(<span class=\"hljs-string\">\"rouge\"</span>)\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">rouge_metric_builder</span>(<span class=\"hljs-params\">tokenizer</span>):</span>\n  <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">compute_rouge_metrics</span>(<span class=\"hljs-params\">pred</span>):</span>\n    <span class=\"hljs-string\">\"\"\"utility to compute ROUGE during training.\"\"\"</span>\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n    <span class=\"hljs-comment\"># All special tokens are removed.</span>\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n    labels_ids[labels_ids == -<span class=\"hljs-number\">100</span>] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n    <span class=\"hljs-comment\"># Compute the metric.</span>\n    rouge_results = rouge_scorer.compute(\n        predictions=pred_str,\n        references=label_str,\n        rouge_types=[<span class=\"hljs-string\">\"rouge2\"</span>, <span class=\"hljs-string\">\"rougeL\"</span>],\n        use_agregator=<span class=\"hljs-literal\">True</span>,\n        use_stemmer=<span class=\"hljs-literal\">False</span>,\n    )\n    <span class=\"hljs-keyword\">return</span> {\n        <span class=\"hljs-string\">\"rouge2\"</span>: <span class=\"hljs-built_in\">round</span>(rouge_results[<span class=\"hljs-string\">'rouge2'</span>].mid.fmeasure, <span class=\"hljs-number\">4</span>),\n        <span class=\"hljs-string\">\"rougeL\"</span>: <span class=\"hljs-built_in\">round</span>(rouge_results[<span class=\"hljs-string\">'rougeL'</span>].mid.fmeasure, <span class=\"hljs-number\">4</span>),\n    }\n  <span class=\"hljs-keyword\">return</span> compute_rouge_metrics\n\nrouge_metric_fn = rouge_metric_builder(tokenizer)</code></pre>\n<p>Fantastic, now all we have to do is set up our trainer class with everything we defined so far and train it!</p>\n<pre><code class=\"hljs language-python\">model = AutoModelForSeq2SeqLM.from_pretrained(<span class=\"hljs-string\">'facebook/bart-base'</span>)\nmodel = model.to(<span class=\"hljs-string\">'cuda:0'</span>)\n\ntrain_args = TrainingArguments(\n    output_dir=<span class=\"hljs-string\">\"BART-commongen\"</span>,\n    do_train=<span class=\"hljs-literal\">True</span>,\n    do_eval=<span class=\"hljs-literal\">True</span>,\n    evaluation_strategy=<span class=\"hljs-string\">\"epoch\"</span>,\n    logging_steps=<span class=\"hljs-number\">100</span>,\n    <span class=\"hljs-comment\"># optimization args, the trainer uses the Adam optimizer</span>\n    <span class=\"hljs-comment\"># and has a linear warmup for the learning rate</span>\n    per_device_train_batch_size=<span class=\"hljs-number\">32</span>,\n    per_device_eval_batch_size=<span class=\"hljs-number\">32</span>,\n    gradient_accumulation_steps=<span class=\"hljs-number\">1</span>,\n    learning_rate=<span class=\"hljs-number\">1e-04</span>,\n    num_train_epochs=<span class=\"hljs-number\">3</span>,\n    warmup_steps=<span class=\"hljs-number\">1000</span>,\n    <span class=\"hljs-comment\"># misc args</span>\n    seed=<span class=\"hljs-number\">42</span>,\n    disable_tqdm=<span class=\"hljs-literal\">False</span>,\n    load_best_model_at_end=<span class=\"hljs-literal\">True</span>,\n    metric_for_best_model=<span class=\"hljs-string\">\"rouge2\"</span>,\n)\n\ntrainer = Seq2SeqTrainer(\n    num_beams=<span class=\"hljs-number\">4</span>,\n    max_length=<span class=\"hljs-number\">32</span>,\n    model=model,\n    args=train_args,\n    train_dataset=train_data_tokenized,\n    eval_dataset=valid_data_tokenized,\n    tokenizer=tokenizer,\n    compute_metrics=rouge_metric_fn,\n)</code></pre>\n<p>And finally:</p>\n<pre><code class=\"hljs language-python\">>> trainer.train()\n\nEpoch\tTraining Loss\tValidation Loss\tRouge2\tRougel\n<span class=\"hljs-number\">1</span>\t<span class=\"hljs-number\">0.981596</span>\t<span class=\"hljs-number\">1.074378</span>\t<span class=\"hljs-number\">0.143700</span>\t<span class=\"hljs-number\">0.344500</span>\n<span class=\"hljs-number\">2</span>\t<span class=\"hljs-number\">0.819494</span>\t<span class=\"hljs-number\">1.053480</span>\t<span class=\"hljs-number\">0.154700</span>\t<span class=\"hljs-number\">0.353600</span>\n<span class=\"hljs-number\">3</span>\t<span class=\"hljs-number\">0.729006</span>\t<span class=\"hljs-number\">1.067981</span>\t<span class=\"hljs-number\">0.156200</span>\t<span class=\"hljs-number\">0.355500</span></code></pre>\n<p>We now have a model that achieves 15.6 ROUGE-2 which can obviously still be tuned, but it is a great starting point.</p>\n<h2 id=\"generating-and-evaluating-predictions\">Generating and evaluating Predictions</h2>\n<p>Given that we now have a model, we also want to generate model outputs now. For this, let's build another two utility functions that generate a batch with only model inputs and which generate and detokenize text with a model.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">make_batch_inputs</span>(<span class=\"hljs-params\">batch, tokenizer, dataset_name, device=<span class=\"hljs-string\">'cuda:0'</span></span>):</span>\n  <span class=\"hljs-string\">\"\"\"\n  Function that takes a batch from a dataset and formats it as input to model.\n  \"\"\"</span>\n  <span class=\"hljs-comment\"># Concatenate the concept names for each example in the batch.</span>\n  input_lists, _ = construct_input_for_batch(batch, dataset_name)\n  <span class=\"hljs-comment\"># Use the model's tokenizer to create the batch input_ids.</span>\n  batch_features = tokenizer(input_lists, padding=<span class=\"hljs-literal\">True</span>, return_tensors=<span class=\"hljs-string\">'pt'</span>)\n  <span class=\"hljs-comment\"># Move all inputs to the device.</span>\n  batch_features = <span class=\"hljs-built_in\">dict</span>([(k, v.to(device)) <span class=\"hljs-keyword\">for</span> k, v <span class=\"hljs-keyword\">in</span> batch_features.items()])\n  <span class=\"hljs-keyword\">return</span> batch_features\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">beam_generate_sentences</span>(<span class=\"hljs-params\">batch,\n                            model,\n                            tokenizer,\n                            dataset_name,\n                            num_beams=<span class=\"hljs-number\">4</span>,\n                            max_length=<span class=\"hljs-number\">32</span>,\n                            device=<span class=\"hljs-string\">'cuda:0'</span></span>):</span>\n  <span class=\"hljs-string\">\"\"\"\n  Function to generate outputs from a model with beam search decoding.\n  \"\"\"</span>\n  <span class=\"hljs-comment\"># Create batch inputs.</span>\n  features = make_batch_inputs(\n      batch=batch,\n      tokenizer=tokenizer,\n      dataset_name=dataset_name,\n      device=device)\n  <span class=\"hljs-comment\"># Generate with beam search.</span>\n  generated_ids = model.generate(\n      input_ids=features[<span class=\"hljs-string\">'input_ids'</span>],\n      attention_mask=features[<span class=\"hljs-string\">'attention_mask'</span>],\n      num_beams=num_beams,\n      max_length=max_length,\n  )\n  <span class=\"hljs-comment\"># Use model tokenizer to decode to text.</span>\n  generated_sentences = [\n      tokenizer.decode(gen_ids.tolist(), skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n      <span class=\"hljs-keyword\">for</span> gen_ids <span class=\"hljs-keyword\">in</span> generated_ids\n  ]\n  <span class=\"hljs-keyword\">return</span> generated_sentences</code></pre>\n<p>We can quickly apply this function across our validation set as a sanity check.</p>\n<pre><code class=\"hljs language-python\">valid_output = data[<span class=\"hljs-string\">'validation'</span>].<span class=\"hljs-built_in\">map</span>(\n    <span class=\"hljs-keyword\">lambda</span> batch: {<span class=\"hljs-string\">'generated'</span>: beam_generate_sentences(\n        batch,\n        model,\n        tokenizer,\n        DATASET_NAME,\n        num_beams=BEAM_SIZE,\n        max_length=MAX_GENERATION_LENGTH)\n    },\n    batched=<span class=\"hljs-literal\">True</span>,\n    batch_size=<span class=\"hljs-number\">128</span>,\n)\n\nrouge_scorer = load_metric(<span class=\"hljs-string\">\"rouge\"</span>)\n<span class=\"hljs-comment\"># Evaluate for ROUGE-2/L</span>\nrouge_results = rouge_scorer.compute(\n    predictions=valid_output[<span class=\"hljs-string\">\"generated\"</span>],\n    references=valid_output[<span class=\"hljs-string\">\"target\"</span>],\n    rouge_types=[<span class=\"hljs-string\">\"rouge2\"</span>, <span class=\"hljs-string\">\"rougeL\"</span>],\n    use_agregator=<span class=\"hljs-literal\">True</span>, use_stemmer=<span class=\"hljs-literal\">False</span>,\n)\n\n<span class=\"hljs-string\">f\"R-2: <span class=\"hljs-subst\">{rouge_results[<span class=\"hljs-string\">'rouge2'</span>].mid.fmeasure:<span class=\"hljs-number\">.3</span>f}</span> R-L: <span class=\"hljs-subst\">{rouge_results[<span class=\"hljs-string\">'rougeL'</span>].mid.fmeasure:<span class=\"hljs-number\">.3</span>f}</span>\"</span></code></pre>\n<p>As expected, this yields the following output:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-string\">'R-2: 0.156 R-L: 0.356'</span></code></pre>\n<h2 id=\"generating-and-submitting-test-predictions\">Generating and Submitting Test Predictions</h2>\n<p>To format your model outputs for GEM, let's first assume that we have the test outputs similar to our validation outputs above:</p>\n<pre><code class=\"hljs language-python\">test_output = data[<span class=\"hljs-string\">'test'</span>].<span class=\"hljs-built_in\">map</span>(\n    <span class=\"hljs-keyword\">lambda</span> batch: {<span class=\"hljs-string\">'generated'</span>: beam_generate_sentences(\n        batch,\n        model,\n        tokenizer,\n        DATASET_NAME,\n        num_beams=BEAM_SIZE,\n        max_length=MAX_GENERATION_LENGTH)\n    },\n    batched=<span class=\"hljs-literal\">True</span>,\n    batch_size=<span class=\"hljs-number\">128</span>,\n)</code></pre>\n<p>Since CommonGen is multi-reference and we are using a deterministic search algorithm, the test data currently holds many duplicates. We need to merge predictions on the concept set ID. To do so, let's define a formatter:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">postprocess_inference_list</span>(<span class=\"hljs-params\">\n    output_list: <span class=\"hljs-built_in\">list</span>,\n    merge_identifier: <span class=\"hljs-built_in\">str</span> = <span class=\"hljs-literal\">None</span>,\n    target_field: <span class=\"hljs-built_in\">str</span> = <span class=\"hljs-literal\">None</span></span>):</span>\n  <span class=\"hljs-string\">\"\"\"Merges duplicate outputs with multiple references.\"\"\"</span>\n  postprocessed_list = []\n  added_identifiers = <span class=\"hljs-built_in\">set</span>()\n  <span class=\"hljs-keyword\">for</span> item <span class=\"hljs-keyword\">in</span> output_list:\n    <span class=\"hljs-keyword\">del</span> item[target_field]\n    <span class=\"hljs-keyword\">if</span> merge_identifier <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span> <span class=\"hljs-literal\">None</span>:\n      <span class=\"hljs-keyword\">if</span> item[merge_identifier] <span class=\"hljs-keyword\">not</span> <span class=\"hljs-keyword\">in</span> added_identifiers:\n        added_identifiers.add(item[merge_identifier])\n      <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-comment\"># Was already added, skip example.</span>\n        <span class=\"hljs-keyword\">continue</span>\n    postprocessed_list.append(item)\n  <span class=\"hljs-keyword\">return</span> postprocessed_list\n\nvalid_formatted = postprocess_inference_list(\n  <span class=\"hljs-built_in\">list</span>(valid_output),\n  merge_identifier=<span class=\"hljs-string\">'concept_set_idx'</span>,\n  target_field=<span class=\"hljs-string\">'target'</span>)\ntest_formatted = postprocess_inference_list(\n  <span class=\"hljs-built_in\">list</span>(test_output),\n  merge_identifier=<span class=\"hljs-string\">'concept_set_idx'</span>,\n  target_field=<span class=\"hljs-string\">'target'</span>)</code></pre>\n<p>In our final step, we only have to add the formatted outputs to our larger submission construct.</p>\n<pre><code class=\"hljs language-python\">submission_dict = {\n    <span class=\"hljs-string\">\"submission_name\"</span>: <span class=\"hljs-string\">'BART-base'</span>,\n    <span class=\"hljs-string\">\"param_count\"</span>: <span class=\"hljs-built_in\">sum</span>(p.numel() <span class=\"hljs-keyword\">for</span> p <span class=\"hljs-keyword\">in</span> model.parameters()),\n    <span class=\"hljs-string\">\"common_gen_val\"</span>: valid_formatted,\n    <span class=\"hljs-string\">\"common_gen_test\"</span>: test_formatted,\n}</code></pre>\n<p>With this, the last step is to write our submission dictionary to a file.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">import</span> json\n<span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(<span class=\"hljs-string\">'gem_submission.json'</span>, <span class=\"hljs-string\">'w'</span>) <span class=\"hljs-keyword\">as</span> f:\n  f.write(json.dumps(submission_dict, indent=<span class=\"hljs-number\">2</span>))</code></pre>\n<h2 id=\"evaluating-your-submission-file-with-the-gem-evaluation-framework\">Evaluating your submission file with the GEM evaluation framework.</h2>\n<p>Obviously, we do not want to rely only on ROUGE scores. For this, we developed the GEM evaluation framework.</p>\n<p>Coming soon :)</p>\n","title":"Getting Started with GEM"}},"__N_SSG":true}