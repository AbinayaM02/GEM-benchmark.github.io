{"pageProps":{"taskData":{"id":"Wizards of Wikipedia","contentHtml":"<h2 id=\"table-of-contents\">Table of Contents</h2>\n<ul>\n<li><a href=\"#dataset-description\">Dataset Description</a>\n<ul>\n<li><a href=\"#dataset-and-task-summary\">Dataset and Task Summary</a></li>\n<li><a href=\"#why-is-this-dataset-part-of-gem\">Why is this dataset part of GEM?</a></li>\n<li><a href=\"#languages\">Languages</a></li>\n</ul>\n</li>\n<li><a href=\"#meta-information\">Meta Information</a>\n<ul>\n<li><a href=\"#dataset-curators\">Dataset Curators</a></li>\n<li><a href=\"#licensing-information\">Licensing Information</a></li>\n<li><a href=\"#citation-information\">Citation Information</a></li>\n<li><a href=\"#leaderboard\">Leaderboard</a></li>\n</ul>\n</li>\n<li><a href=\"#dataset-structure\">Dataset Structure</a>\n<ul>\n<li><a href=\"#data-instances\">Data Instances</a></li>\n<li><a href=\"#data-fields\">Data Fields</a></li>\n<li><a href=\"#data-statistics\">Data Statistics</a></li>\n</ul>\n</li>\n<li><a href=\"#dataset-creation\">Dataset Creation</a>\n<ul>\n<li><a href=\"#curation-rationale\">Curation Rationale</a></li>\n<li><a href=\"#communicative-goal\">Communicative Goal</a></li>\n<li><a href=\"#source-data\">Source Data</a>\n<ul>\n<li><a href=\"#initial-data-collection-and-normalization\">Initial Data Collection and Normalization</a></li>\n</ul>\n</li>\n<li><a href=\"#annotations\">Annotations</a>\n<ul>\n<li><a href=\"#annotation-process\">Annotation process</a></li>\n<li><a href=\"#who-are-the-annotators\">Who are the annotators?</a></li>\n</ul>\n</li>\n<li><a href=\"#personal-and-sensitive-information\">Personal and Sensitive Information</a></li>\n</ul>\n</li>\n<li><a href=\"#changes-to-the-original-dataset-for-gem\">Changes to the Original Dataset for GEM</a></li>\n<li><a href=\"#considerations-for-using-the-data\">Considerations for Using the Data</a>\n<ul>\n<li><a href=\"#social-impact-of-the-dataset\">Social Impact of the Dataset</a></li>\n<li><a href=\"#impact-on-underserved-communities\">Impact on Underserved Communities</a></li>\n<li><a href=\"#discussion-of-biases\">Discussion of Biases</a></li>\n<li><a href=\"#other-known-limitations\">Other Known Limitations</a></li>\n</ul>\n</li>\n<li><a href=\"#getting-started-with-in-depth-research-on-the-task\">Getting started with in-depth research on the task</a></li>\n</ul>\n<h2 id=\"dataset-description\">Dataset Description</h2>\n<ul>\n<li><strong>Homepage:</strong> <a href=\"https://parl.ai/projects/wizard_of_wikipedia/\">Wizard of Wikipedia Homepage</a></li>\n<li><strong>Repository:</strong> <a href=\"https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/wizard_of_wikipedia\">Wizard of Wikipedia task in PARLAI repo</a></li>\n<li><strong>Paper:</strong> [Wizard of Wikipedia: Knowledge-Powered Conversational agents</li>\n</ul>\n<p>](<a href=\"https://arxiv.org/abs/1811.01241\">https://arxiv.org/abs/1811.01241</a>)</p>\n<ul>\n<li><strong>Point of Contact:</strong> Option 1: Contacting via creating an issue on PARLAI <a href=\"https://github.com/facebookresearch/ParlAI\">github repo</a>. Option 2:</li>\n</ul>\n<p><a href=\"%7Bedinan,roller,kshuster,angelafan,michaelauli,jase%7D@fb.com\">Contact authors on the above paper</a></p>\n<h3 id=\"dataset-and-task-summary\">Dataset and Task Summary</h3>\n<p>Task Summary:\nThe Wizard of Wikipedia is an open-domain dialogue task for training agents that can converse knowledgably about open-domain topics. In open-domain dialogue intelligent agents should exhibit the use of knowledge. This could simply be done by training models which \"generate and hope\" generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output. Through grounded dialog datasets, one could rather train models that employing recalled knowledge as context. The goal here is to retrieving knowledge (relevant to context) from wikipedia, reading and conditioning on it, and finally generating natural responses.</p>\n<p>Dataset:\nThe dataset consists of conversations grounded with knowledge retrieved from Wikipedia. It contains 201k utterances from 22k dialogues spanning over 1300 diverse topics, split into train, test, and valid sets. The associated Wikipedia knowledge base has 5.4M articles and 93M sentences.</p>\n<h3 id=\"why-is-this-dataset-part-of-gem\">Why is this dataset part of GEM?</h3>\n<p>Wizard-of-Wikipedia is one of the three datasets representing Dialog Generation NLG in GEM.</p>\n<h3 id=\"languages\">Languages</h3>\n<p>Contains English text only</p>\n<h2 id=\"meta-information\">Meta Information</h2>\n<h3 id=\"dataset-curators\">Dataset Curators</h3>\n<p>The dataset was curated by a team of researchers from Facebook: Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston.</p>\n<h3 id=\"licensing-information\">Licensing Information</h3>\n<p>The <a href=\"https://parl.ai/projects/wizard_of_wikipedia/\">project page</a> does not mention a license for the dataset. ParlAI repository has <a href=\"https://github.com/facebookresearch/ParlAI/blob/master/LICENSE\">MIT License</a></p>\n<h3 id=\"citation-information\">Citation Information</h3>\n<pre><code>@inproceedings{dinan2019wizard,\n  author={Emily Dinan and Stephen Roller and Kurt Shuster and Angela Fan and Michael Auli and Jason Weston},\n  title={{W}izard of {W}ikipedia: Knowledge-powered Conversational Agents},\n  booktitle = {Proceedings of the International Conference on Learning Representations (ICLR)},\n  year={2019},\n}\n</code></pre>\n<h3 id=\"leaderboard\">Leaderboard</h3>\n<p>Project page has a <a href=\"https://parl.ai/projects/wizard_of_wikipedia/\">leaderboard</a>.</p>\n<h2 id=\"dataset-structure\">Dataset Structure</h2>\n<h3 id=\"data-instances\">Data Instances</h3>\n<p>Data example (Source: Running <code>parlai display_data -t wizard_of_wikipedia -dt train</code> after installing ParlAI library)</p>\n<pre><code>chosen_topic: Science fiction \nA: I think science fiction is an amazing genre for anything. Future science, technology, time travel, FTL travel, they're all such interesting concepts.\nW: I'm a huge fan of science fiction myself! \nA: Awesome! I really love how sci-fi storytellers focus on political/social/philosophical issues that would still be around even in the future. Makes them relatable.\nW: I agree. One of my favorite forms of science fiction is anything related to time travel! I find it fascinating.\nA: It's not quite sci-fi, but my favorite version of time travel is in Harry Potter and the Prisoner of Azkaban. Breaks zero logical rules.\nW: And that's difficult to do when dealing with time travel. I actually haven't seen the latest Harry Potter movies. Guess it's time to check them out!\nA: If you really want a look at the potential negative consequences of scientific innovation, what you should check out is the TV show Fringe. Incredibly well written.\n</code></pre>\n<h3 id=\"data-fields\">Data Fields</h3>\n<p>Brief description of some of the fields:</p>\n<ul>\n<li><code>chosen_topic</code>: topic of dialog</li>\n<li><code>checked_sentence</code>: selected knowledge</li>\n<li><code>actor_id</code>: 'apprentice' or 'wizard'</li>\n<li><code>text</code>: dialog utterance text</li>\n</ul>\n<p>Thr original repository has more <a href=\"https://github.com/facebookresearch/ParlAI/blob/master/parlai/tasks/wizard_of_wikipedia/worlds.py\">details</a></p>\n<h3 id=\"data-statistics\">Data Statistics</h3>\n<p>Some statistics for train split:</p>\n<ul>\n<li>Number of Utterances : 166,787</li>\n<li>Number of Dialogues: 18,430</li>\n<li>Number of Topics: 1,247</li>\n<li>Average Turns per Dialogue: 9.0</li>\n</ul>\n<h2 id=\"dataset-creation\">Dataset Creation</h2>\n<h3 id=\"curation-rationale\">Curation Rationale</h3>\n<p>As mentioned in the Wizard-of-Wikipedia paper, goal of collecting data of wizard-apprentice conversations between humans is to then be able to train models which can replace the human wizard and speak to a human apprentice, similar to the procedure in Wizard of Oz experiments.</p>\n<h3 id=\"communicative-goal\">Communicative Goal</h3>\n<p>Apprentice talks to the wizard freely, playing the role of a curious learner, eager to chat.\nHuman Wizard is asked discuss a topic with an eager apprentice.</p>\n<h3 id=\"source-data\">Source Data</h3>\n<p>A set of 1365 natural, open-domain dialogue topics, each linked to a Wikipedia article, was crowd-sourced. These dialogues are on diverse topics such as commuting, Gouda cheese, music festivals, podcasts, and bowling.</p>\n<p>At each step of the dialogue the wizard has access to a set of passages of knowledge which may be relevant to the given dialogue context. The top 7 articles (first paragraph only) for the last two turns of dialogue (by wizard and apprentice) and the article (first 10 sentences only) for the original topic, and present these articles to the wizard as knowledge context, along with their titles.</p>\n<h4 id=\"initial-data-collection-and-normalization\">Initial Data Collection and Normalization</h4>\n<p>The final dialogue dataset consists of 22,311 dialogues with\n201,999 turns, which is divided into 166,787 for train, 17,715 for validation, and 17,497 for test. The test set is split into two subsets, Test Seen and Test Unseen. Test Seen contains 533 overlapping topics with the training set with new dialogues about those topics. Test Unseen consists of 58 topics never seen before in train or validation.</p>\n<h3 id=\"annotations\">Annotations</h3>\n<h4 id=\"annotation-process\">Annotation process</h4>\n<p>Authors stated the following annotation procedure in their paper:</p>\n<ol>\n<li>Either the wizard or apprentice is picked to choose the topic and speak first. The other</li>\n</ol>\n<p>player receives the topic information, and the conversation begins.\n2. When the apprentice sends the wizard a message, the wizard is shown relevant knowledge\n(described below), and chooses a relevant sentence in order to construct a response, or else\nchooses the no sentence used option.\n3. The Wizard responds to the apprentice basing their response on their chosen sentence.\n4. The conversation repeats until one of the conversation partners ends the chat (after a minimum of 4 or 5 turns each, randomly chosen beforehand).</p>\n<h4 id=\"who-are-the-annotators\">Who are the annotators?</h4>\n<p>Crowdworkers were recruited for data collection. More details about crowdworkers are not provided in the paper.</p>\n<h3 id=\"personal-and-sensitive-information\">Personal and Sensitive Information</h3>\n<p>[N/A]</p>\n<h2 id=\"changes-to-the-original-dataset-for-gem\">Changes to the Original Dataset for GEM</h2>\n<p>None at present</p>\n<h2 id=\"considerations-for-using-the-data\">Considerations for Using the Data</h2>\n<h3 id=\"social-impact-of-the-dataset\">Social Impact of the Dataset</h3>\n<p>The dataset would probably have similar societal impacts as most other open domain dialog datasets. There are several resources available for social impact of open domain dialog agents (such as <a href=\"https://dl.acm.org/doi/10.1145/3170427.3185372\">SIG: chatbots for social good. Følstad et al 2018</a>).</p>\n<h3 id=\"impact-on-underserved-communities\">Impact on Underserved Communities</h3>\n<p>The dataset is in English language only.\nThe set of seed topics used, though diverse, might not be representative of all communities.</p>\n<h3 id=\"discussion-of-biases\">Discussion of Biases</h3>\n<p>The topics are crowdsourced, and maybe limited to topics relevant to only some communities.\nMoreover, dialogues are collected through crowdsourcing, and may exhibit biased opinions on various topics.</p>\n<h3 id=\"other-known-limitations\">Other Known Limitations</h3>\n<h2 id=\"getting-started-with-in-depth-research-on-the-task\">Getting started with in-depth research on the task</h2>\n<p>Some useful resources:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1811.01241\">Link to paper</a></li>\n<li><a href=\"https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/wizard_of_wikipedia\">ParlAI code for Wizards of Wikipedia</a></li>\n<li><a href=\"https://huggingface.co/datasets/woz_dialogue#social-impact-of-dataset\">Huggingface datasets</a></li>\n</ul>\n","title":"Wizards of Wikipedia","type":"Dialog","motivation":"Knowledge conditioned open domain dialog."}},"__N_SSG":true}