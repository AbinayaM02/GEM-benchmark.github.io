{
  "common_metrics": {
    "rouge1.fmeasure": {
      "citation": "",
      "description": "ROUGE score focusing on unigrams.",
      "show_as": "ROUGE-1"
    },
    "rouge2.fmeasure": {
      "citation": "",
      "description": "ROUGE score focusing on bigrams.",
      "show_as": "ROUGE-2"
    },
    "rougeL.fmeasure": {
      "citation": "",
      "description": "ROUGE score focusing on longest common subsequence.",
      "show_as": "ROUGE-L"
    },
    "msttr-100": {
      "citation": "",
      "description": "Mean Segmental Type-Token Ratio, a measure of lexical diversity.",
      "show_as": "MSTTR"
    },
    "mean_pred_length": {
      "citation": "",
      "description": "Average length of a system output.",
      "show_as": "Output Length"
    },
    "distinct-1": {
      "citation": "",
      "description": "Ratio of distinct unigrams / total number of unigrams.",
      "show_as": "Distinct-1"
    },
    "vocab_size-1": {
      "citation": "",
      "description": "Number of distinct words used by the system.",
      "show_as": "Vocabulary Size"
    },
    "unique-1": {
      "citation": "",
      "description": "number of unigrams that only occur once in the whole data.",
      "show_as": "Unique-1"
    },
    "entropy-1": {
      "citation": "",
      "description": "Shannon entropy over unigrams",
      "show_as": "Entropy-1"
    },
    "distinct-2": {
      "citation": "",
      "description": "Ratio of distinct bigrams / total number of bigrams.",
      "show_as": "Distinct-2"
    },
    "vocab_size-2": {
      "citation": "",
      "description": "Number of distinct bigrams used by the system.",
      "show_as": "Bigram Vocabulary Size"
    },
    "unique-2": {
      "citation": "",
      "description": "Number of bigrams that only occur once in the whole data",
      "show_as": "Unique-2"
    },
    "entropy-2": {
      "citation": "",
      "description": "Shannon entropy over bigrams.",
      "show_as": "Entropy-2"
    },
    "cond_entropy-2": {
      "citation": "",
      "description": "Language model style conditional entropy -- N-grams conditioned on N-1-grams.",
      "show_as": "Bigram Conditional Entropy"
    },
    "distinct-3": {
      "citation": "",
      "description": "Ratio of distinct trigrams / total number of trigrams.",
      "show_as": "Distinct-3"
    },
    "vocab_size-3": {
      "citation": "",
      "description": "Number of distinct trigrams used by the system.",
      "show_as": "Trigram Vocabulary Size"
    },
    "unique-3": {
      "citation": "",
      "description": "Number of trigrams that only occur once in the whole data.",
      "show_as": "Unique-3"
    },
    "entropy-3": {
      "citation": "",
      "description": "Shannon entropy over trigrams.",
      "show_as": "Entropy-2"
    },
    "cond_entropy-3": {
      "citation": "",
      "description": "Language model style conditional entropy -- N-grams conditioned on N-1-grams",
      "show_as": "Trigram Conditional Entropy"
    },
    "bertscore.f1": {
      "citation": "",
      "description": "A BERT-based similarity measure between reference and generation.",
      "show_as": "BERTScore"
    },
    "bleu": {
      "citation": "",
      "description": "A measure of lexical similarity.",
      "show_as": "BLEU"
    },
    "bleurt": {
      "citation": "",
      "description": "A learned metric to measure semantic equivalence between reference and generation.",
      "show_as": "BLEURT"
    },
    "meteor": {
      "citation": "",
      "description": "An advanced lexical similarity metric also including stemming and synonymy matching.",
      "show_as": "Meteor"
    },
    "nubia.semantic_relation": {
      "citation": "",
      "description": "A sentence similarity score on a scale of 5 describing how close in meaning 2 sentences are.",
      "show_as": "NUBIA (Similarity)"
    },
    "nubia.logical_agreement": {
      "citation": "",
      "description": "Likelihood (out of 100) of 2 sentences being in logical agreement.",
      "show_as": "NUBIA (Entailment)"
    },
    "nubia.grammar_hyp": {
      "citation": "",
      "description": "Language Model Perplexity on Hypothesis sentence.",
      "show_as": "NUBIA (Grammaticality)"
    },
    "nubia.nubia_score": {
      "citation": "",
      "description": "A metric combining all other aspects, normalized to [0,1].",
      "show_as": "NUBIA (Overall)"
    },
    "sari": {
      "citation": "",
      "description": "A simplification metric.",
      "show_as": "SARI"
    },
    "nist": {
      "citation": "http://dl.acm.org/citation.cfm?id=1289189.1289273",
      "description": "NIST is an alternative to BLEU with slightly different calculation.",
      "show_as": "NIST"
    },
    "local_recall.1": {
      "citation": "https://www.aclweb.org/anthology/C18-1147/",
      "description": "LocalRecall checks the extent to which a model produces the same tokens as the reference data.",
      "show_as": "Local Unigram Recall"
    },
    "local_recall.2": {
      "citation": "https://www.aclweb.org/anthology/C18-1147/",
      "description": "LocalRecall checks the extent to which a model produces the same tokens as the reference data.",
      "show_as": "Local Bigram Recall"
    }
  },
  "challenges": {
    "data2text": {
      "datasets": [
        "common_gen_val",
        "dart_val",
        "e2e_nlg_val",
        "totto_val",
        "cs_restaurants_val",
        "web_nlg_en_val",
        "web_nlg_ru_val"
      ],
      "metrics": []
    },
    "summarization": {
      "datasets": [
        "mlsum_de_val",
        "mlsum_es_val",
        "xsum_val",
        "wiki_lingua_turkish_tr_val",
        "wiki_lingua_vietnamese_vi_val",
        "wiki_lingua_spanish_es_val",
        "wiki_lingua_russian_ru_val"
      ],
      "metrics": []
    },
    "dialog": {
      "datasets": [
        "schema_guided_dialog_val"
      ],
      "metrics": []
    },
    "simplification": {
      "datasets": [
        "wiki_auto_asset_turk_test_asset",
        "wiki_auto_asset_turk_test_turk"
      ],
      "metrics": []
    }
  },
  "measures": {
    "diversity": [
      "msttr-100",
      "distinct-1",
      "distinct-2",
      "unique-1",
      "unique-2",
      "entropy-1",
      "entropy-2",
      "cond_entropy-2"
    ],
    "lexical": [
      "rouge1.fmeasure",
      "rouge2.fmeasure",
      "rougeL.fmeasure",
      "bleu",
      "meteor",
      "sari",
      "nist",
      "local_recall.1",
      "local_recall.2"
    ],
    "semantic": [
      "bertscore.f1",
      "bleurt"
    ],
    "faithful": [
      "nubia.semantic_relation",
      "nubia.logical_agreement",
      "nubia.grammar_hyp",
      "nubia.nubia_score"
    ],
    "descriptive": [
      "mean_pred_length",
      "vocab_size-1",
      "vocab_size-2"
    ]
  }
}